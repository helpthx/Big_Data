{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Linear Methods\n",
    "\n",
    "\n",
    "After completing this lesson you should be able to:\n",
    "\n",
    "* Understand the Pipeline API for Logistic Regression and Linear Least Squares\n",
    "* Perform classification with Logistic Regression\n",
    "* Perform regression with Linear Least Squares\n",
    "* Use regularization with Logistic Regression and Linear Least Squares\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "* Widely used to predict binary responses\n",
    "* Can be generalized into multinomial logistic regression\n",
    "\n",
    "The benefits of Logistic Regression are:\n",
    "\n",
    "* there are no tuning parameters\n",
    "* the prediction equation is simple and easy to implement\n",
    "\n",
    "## Continuing from previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@7b924414\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.util.MLUtils.{convertVectorColumnsFromML=>fromML, convertVectorColumnsToML=>toML}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@7b924414"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.mllib.util.MLUtils.{\n",
    "  convertVectorColumnsFromML => fromML,\n",
    "  convertVectorColumnsToML => toML\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [label: double, features: vector]\n",
       "splitData = Array([label: double, features: vector], [label: double, features: vector])\n",
       "trainingData = [label: double, features: vector]\n",
       "testData = [label: double, features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.util.MLUtils\n",
    " \n",
    "val data = toML(MLUtils.loadLibSVMFile(sc, \"/resources/data/sample_libsvm_data.txt\").toDF())\n",
    "\n",
    "val splitData = data.randomSplit(Array(0.7, 0.3))\n",
    "val trainingData = toML(splitData(0))\n",
    "val testData = toML(splitData(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,243,244,262,263,271,272,289,290,300,317,328,356,372,378,385,399,400,405,406,407,427,428,433,434,455,456,461,462,483,484,489,490,511,512,517,539,540,567,568,596],[-1.3886039630068322E-4,-1.3843938227317773E-4,-2.3274331562427958E-4,-5.511691204943551E-5,-2.3254539357946662E-4,-5.5779485773802486E-5,-2.621346009654944E-4,-4.41170392889009E-5,-1.966054170694689E-4,-3.8558594518799106E-4,-6.604319534006298E-5,-1.8879593209464936E-4,-4.726823787945629E-5,-3.381633606932521E-5,5.965157617133719E-4,-4.0083724406133714E-5,-2.582019292149748E-5,-3.296873365606878E-5,1.0553514223711934E-4,7.132333431127835E-4,9.451095527271452E-5,-2.2619209985035316E-5,-6.829261041600189E-5,6.904002687636397E-4,6.95973655426369E-4,-9.405394470679965E-5,-7.149010995687509E-5,6.146723620461247E-4,6.264392360691484E-4,-3.902726221278654E-4,-5.137849592884832E-5,2.6928119476221684E-4,1.053790224622655E-4,-4.3452068436612554E-4,-1.7831112968286745E-4,1.2096902537491854E-4,-4.033583524218695E-4,-2.954586990840618E-4,-6.064022078026638E-5,-2.7017666930740156E-4,-4.5269852040028135E-5])\n",
      "Intercept: 0.15069046066389336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logr = logreg_103154851f7a\n",
       "logrModel = LogisticRegressionModel: uid = logreg_103154851f7a, numClasses = 2, numFeatures = 692\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = logreg_103154851f7a, numClasses = 2, numFeatures = 692"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n",
    "\n",
    "val logr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "val logrModel = logr.fit(trainingData)\n",
    "\n",
    "println(s\"Weights: ${logrModel.coefficients}\\nIntercept: ${logrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D@4c80199e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainingSummaryLR = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@57110714\n",
       "objectiveHistoryLR = Array(0.6896709283570267, 0.670361515592167, 0.62568954069663, 0.6158425130491062, 0.6079859822177494, 0.6053072036877187, 0.6008503738735869, 0.5981344029865898, 0.595946695809437, 0.593210247979345, 0.5911804413730352)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(0.6896709283570267, 0.670361515592167, 0.62568954069663, 0.6158425130491062, 0.6079859822177494, 0.6053072036877187, 0.6008503738735869, 0.5981344029865898, 0.595946695809437, 0.593210247979345, 0.5911804413730352)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSummaryLR = logrModel.summary\n",
    "val objectiveHistoryLR = trainingSummaryLR.objectiveHistory\n",
    "println(objectiveHistoryLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least Squares\n",
    "\n",
    "- Most common formulation for regression problems\n",
    "- Average loss = Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "val lrModel = lr.fit(trainingData)\n",
    "\n",
    "println(s\"Weights: ${lrModel.coefficients}\\nIntercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val trainingSummaryLLS = lrModel.summary\n",
    "val objectiveHistoryLLS = trainingSummaryLLS.objectiveHistory\n",
    "\n",
    "println(objectiveHistoryLLS)\n",
    "\n",
    "trainingSummaryLLS.residuals.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "Having completed this lesson, you should be able to:\n",
    "\n",
    "- Understand the Pipelines API for Logistic Regression and Linear Least Squares\n",
    "- Perform classification with Logistic Regression\n",
    "- Perform classification with Linear Least Squares\n",
    "- Use regularization with Logistic Regression and Linear Least Squares\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
